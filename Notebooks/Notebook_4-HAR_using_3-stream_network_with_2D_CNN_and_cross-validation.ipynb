{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":478077,"sourceType":"datasetVersion","datasetId":222223}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom torch.nn import functional as F\nfrom torch.utils.data._utils.collate import default_collate\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:29.994767Z","iopub.execute_input":"2024-12-12T10:29:29.997000Z","iopub.status.idle":"2024-12-12T10:29:36.426032Z","shell.execute_reply.started":"2024-12-12T10:29:29.996955Z","shell.execute_reply":"2024-12-12T10:29:36.425183Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Directory paths\nVIDEO_DIR = \"/kaggle/input/ucf11-action-recognize/UCF11_updated_mpg\"\nOUTPUT_DIR = \"/kaggle/working/temp_folder\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:36.427481Z","iopub.execute_input":"2024-12-12T10:29:36.427877Z","iopub.status.idle":"2024-12-12T10:29:36.431884Z","shell.execute_reply.started":"2024-12-12T10:29:36.427849Z","shell.execute_reply":"2024-12-12T10:29:36.431013Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"categories = os.listdir(VIDEO_DIR)\ncategories","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:36.433074Z","iopub.execute_input":"2024-12-12T10:29:36.433415Z","iopub.status.idle":"2024-12-12T10:29:36.449962Z","shell.execute_reply.started":"2024-12-12T10:29:36.433374Z","shell.execute_reply":"2024-12-12T10:29:36.448985Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['biking',\n 'trampoline_jumping',\n 'swing',\n 'walking',\n 'golf_swing',\n 'soccer_juggling',\n 'tennis_swing',\n 'volleyball_spiking',\n 'basketball',\n 'horse_riding',\n 'diving']"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"num_classes = len(categories)\nnum_classes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:36.451988Z","iopub.execute_input":"2024-12-12T10:29:36.452250Z","iopub.status.idle":"2024-12-12T10:29:36.457468Z","shell.execute_reply.started":"2024-12-12T10:29:36.452224Z","shell.execute_reply":"2024-12-12T10:29:36.456508Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"11"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"def create_dataset(input_folder):\n    groups = []\n    classes = os.listdir(input_folder)\n    index = 0\n    for class_name in sorted(classes):\n        print(f'Extracting Data of Class: {class_name}')\n        \n        label_folder_path = os.path.join(input_folder, class_name)\n        if os.path.isdir(label_folder_path):\n            group_folders = os.listdir(label_folder_path)\n            for group_folder in group_folders:\n                if group_folder != 'Annotation':\n                    video_dir = os.path.join(label_folder_path,group_folder)\n                    for video_file in os.listdir(video_dir):\n                        groups.append([os.path.join(video_dir, video_file), class_name])\n            index += 1\n    return groups","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:36.458354Z","iopub.execute_input":"2024-12-12T10:29:36.458610Z","iopub.status.idle":"2024-12-12T10:29:36.468361Z","shell.execute_reply.started":"2024-12-12T10:29:36.458580Z","shell.execute_reply":"2024-12-12T10:29:36.467502Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"groups = create_dataset(VIDEO_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:36.469364Z","iopub.execute_input":"2024-12-12T10:29:36.469656Z","iopub.status.idle":"2024-12-12T10:29:37.757415Z","shell.execute_reply.started":"2024-12-12T10:29:36.469617Z","shell.execute_reply":"2024-12-12T10:29:37.756475Z"}},"outputs":[{"name":"stdout","text":"Extracting Data of Class: basketball\nExtracting Data of Class: biking\nExtracting Data of Class: diving\nExtracting Data of Class: golf_swing\nExtracting Data of Class: horse_riding\nExtracting Data of Class: soccer_juggling\nExtracting Data of Class: swing\nExtracting Data of Class: tennis_swing\nExtracting Data of Class: trampoline_jumping\nExtracting Data of Class: volleyball_spiking\nExtracting Data of Class: walking\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"groups[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:37.758542Z","iopub.execute_input":"2024-12-12T10:29:37.758897Z","iopub.status.idle":"2024-12-12T10:29:37.764444Z","shell.execute_reply.started":"2024-12-12T10:29:37.758870Z","shell.execute_reply":"2024-12-12T10:29:37.763604Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"[['/kaggle/input/ucf11-action-recognize/UCF11_updated_mpg/basketball/v_shooting_05/v_shooting_05_01.mpg',\n  'basketball'],\n ['/kaggle/input/ucf11-action-recognize/UCF11_updated_mpg/basketball/v_shooting_05/v_shooting_05_04.mpg',\n  'basketball'],\n ['/kaggle/input/ucf11-action-recognize/UCF11_updated_mpg/basketball/v_shooting_05/v_shooting_05_03.mpg',\n  'basketball'],\n ['/kaggle/input/ucf11-action-recognize/UCF11_updated_mpg/basketball/v_shooting_05/v_shooting_05_02.mpg',\n  'basketball'],\n ['/kaggle/input/ucf11-action-recognize/UCF11_updated_mpg/basketball/v_shooting_09/v_shooting_09_05.mpg',\n  'basketball']]"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"len(groups)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:37.765436Z","iopub.execute_input":"2024-12-12T10:29:37.765697Z","iopub.status.idle":"2024-12-12T10:29:37.777216Z","shell.execute_reply.started":"2024-12-12T10:29:37.765672Z","shell.execute_reply":"2024-12-12T10:29:37.776415Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"1600"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Convert to DataFrame\ndf = pd.DataFrame(groups, columns=[\"videos\", \"Category\"])\n\n# Display the DataFrame\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:37.778200Z","iopub.execute_input":"2024-12-12T10:29:37.778470Z","iopub.status.idle":"2024-12-12T10:29:37.801632Z","shell.execute_reply.started":"2024-12-12T10:29:37.778446Z","shell.execute_reply":"2024-12-12T10:29:37.800805Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                                 videos    Category\n0     /kaggle/input/ucf11-action-recognize/UCF11_upd...  basketball\n1     /kaggle/input/ucf11-action-recognize/UCF11_upd...  basketball\n2     /kaggle/input/ucf11-action-recognize/UCF11_upd...  basketball\n3     /kaggle/input/ucf11-action-recognize/UCF11_upd...  basketball\n4     /kaggle/input/ucf11-action-recognize/UCF11_upd...  basketball\n...                                                 ...         ...\n1595  /kaggle/input/ucf11-action-recognize/UCF11_upd...     walking\n1596  /kaggle/input/ucf11-action-recognize/UCF11_upd...     walking\n1597  /kaggle/input/ucf11-action-recognize/UCF11_upd...     walking\n1598  /kaggle/input/ucf11-action-recognize/UCF11_upd...     walking\n1599  /kaggle/input/ucf11-action-recognize/UCF11_upd...     walking\n\n[1600 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>videos</th>\n      <th>Category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/ucf11-action-recognize/UCF11_upd...</td>\n      <td>basketball</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/ucf11-action-recognize/UCF11_upd...</td>\n      <td>basketball</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/ucf11-action-recognize/UCF11_upd...</td>\n      <td>basketball</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/ucf11-action-recognize/UCF11_upd...</td>\n      <td>basketball</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/ucf11-action-recognize/UCF11_upd...</td>\n      <td>basketball</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1595</th>\n      <td>/kaggle/input/ucf11-action-recognize/UCF11_upd...</td>\n      <td>walking</td>\n    </tr>\n    <tr>\n      <th>1596</th>\n      <td>/kaggle/input/ucf11-action-recognize/UCF11_upd...</td>\n      <td>walking</td>\n    </tr>\n    <tr>\n      <th>1597</th>\n      <td>/kaggle/input/ucf11-action-recognize/UCF11_upd...</td>\n      <td>walking</td>\n    </tr>\n    <tr>\n      <th>1598</th>\n      <td>/kaggle/input/ucf11-action-recognize/UCF11_upd...</td>\n      <td>walking</td>\n    </tr>\n    <tr>\n      <th>1599</th>\n      <td>/kaggle/input/ucf11-action-recognize/UCF11_upd...</td>\n      <td>walking</td>\n    </tr>\n  </tbody>\n</table>\n<p>1600 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Map category names to numeric labels\nclass_mapping = {category: idx for idx, category in enumerate(categories)}\n\nprint(\"Category to Label Mapping:\")\nclass_mapping","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:37.805161Z","iopub.execute_input":"2024-12-12T10:29:37.805448Z","iopub.status.idle":"2024-12-12T10:29:37.812404Z","shell.execute_reply.started":"2024-12-12T10:29:37.805420Z","shell.execute_reply":"2024-12-12T10:29:37.811459Z"}},"outputs":[{"name":"stdout","text":"Category to Label Mapping:\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'biking': 0,\n 'trampoline_jumping': 1,\n 'swing': 2,\n 'walking': 3,\n 'golf_swing': 4,\n 'soccer_juggling': 5,\n 'tennis_swing': 6,\n 'volleyball_spiking': 7,\n 'basketball': 8,\n 'horse_riding': 9,\n 'diving': 10}"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"### Label Encoding","metadata":{}},{"cell_type":"code","source":"df['label'] = df['Category'].map(class_mapping)\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:37.813394Z","iopub.execute_input":"2024-12-12T10:29:37.813743Z","iopub.status.idle":"2024-12-12T10:29:37.833012Z","shell.execute_reply.started":"2024-12-12T10:29:37.813702Z","shell.execute_reply":"2024-12-12T10:29:37.831908Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                                 videos    Category  label\n0     /kaggle/input/ucf11-action-recognize/UCF11_upd...  basketball      8\n1     /kaggle/input/ucf11-action-recognize/UCF11_upd...  basketball      8\n2     /kaggle/input/ucf11-action-recognize/UCF11_upd...  basketball      8\n3     /kaggle/input/ucf11-action-recognize/UCF11_upd...  basketball      8\n4     /kaggle/input/ucf11-action-recognize/UCF11_upd...  basketball      8\n...                                                 ...         ...    ...\n1595  /kaggle/input/ucf11-action-recognize/UCF11_upd...     walking      3\n1596  /kaggle/input/ucf11-action-recognize/UCF11_upd...     walking      3\n1597  /kaggle/input/ucf11-action-recognize/UCF11_upd...     walking      3\n1598  /kaggle/input/ucf11-action-recognize/UCF11_upd...     walking      3\n1599  /kaggle/input/ucf11-action-recognize/UCF11_upd...     walking      3\n\n[1600 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>videos</th>\n      <th>Category</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/ucf11-action-recognize/UCF11_upd...</td>\n      <td>basketball</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/ucf11-action-recognize/UCF11_upd...</td>\n      <td>basketball</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/ucf11-action-recognize/UCF11_upd...</td>\n      <td>basketball</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/ucf11-action-recognize/UCF11_upd...</td>\n      <td>basketball</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/ucf11-action-recognize/UCF11_upd...</td>\n      <td>basketball</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1595</th>\n      <td>/kaggle/input/ucf11-action-recognize/UCF11_upd...</td>\n      <td>walking</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1596</th>\n      <td>/kaggle/input/ucf11-action-recognize/UCF11_upd...</td>\n      <td>walking</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1597</th>\n      <td>/kaggle/input/ucf11-action-recognize/UCF11_upd...</td>\n      <td>walking</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1598</th>\n      <td>/kaggle/input/ucf11-action-recognize/UCF11_upd...</td>\n      <td>walking</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1599</th>\n      <td>/kaggle/input/ucf11-action-recognize/UCF11_upd...</td>\n      <td>walking</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>1600 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"# Video Preprocessing\n\n### Extracting RGB Frames and Optical Flow","metadata":{}},{"cell_type":"code","source":"def extract_frames(video_path, frame_folder, SEQUENCE_LENGTH = 10):\n    cap = cv2.VideoCapture(video_path)\n    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    skip_frames_window = max(int(frame_count / SEQUENCE_LENGTH), 1)\n    frames = []\n    \n    for i in range(SEQUENCE_LENGTH):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, i * skip_frames_window)\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frames.append(frame)\n        cv2.imwrite(f\"{frame_folder}/frame_{i}.jpg\", frame)\n    \n    cap.release()\n\n    if not frames:\n        raise ValueError(f\"No frames found in video: {video_path}\")\n         \n    return frames","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:37.834366Z","iopub.execute_input":"2024-12-12T10:29:37.834792Z","iopub.status.idle":"2024-12-12T10:29:37.845617Z","shell.execute_reply.started":"2024-12-12T10:29:37.834748Z","shell.execute_reply":"2024-12-12T10:29:37.844499Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def calculate_optical_flow(frames):\n    if len(frames) < 2:\n        raise ValueError(\"Insufficient frames for optical flow calculation.\")\n        \n    flow_frames = []\n    prev_frame = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)\n    \n    for i in range(1, len(frames)):\n        next_frame = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n        flow = cv2.calcOpticalFlowFarneback(prev_frame, next_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n        flow_frames.append(flow)\n        prev_frame = next_frame\n    \n    return flow_frames","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:37.846894Z","iopub.execute_input":"2024-12-12T10:29:37.847633Z","iopub.status.idle":"2024-12-12T10:29:37.861504Z","shell.execute_reply.started":"2024-12-12T10:29:37.847591Z","shell.execute_reply":"2024-12-12T10:29:37.860749Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Dataset Preparation\n\n### Define a custom dataset class that loads RGB frames, optical flow, and labels.","metadata":{}},{"cell_type":"code","source":"class VideoDataset(Dataset):\n    def __init__(self, data, labels, transform=False, rgb_transform=None, flow_transform=None):\n        \"\"\"\n        Args:\n            dataframe (pd.DataFrame): DataFrame containing video paths and labels.\n            transform (callable, optional): Optional transform to be applied on frames.\n        \"\"\"\n        # self.dataframe = dataframe\n        self.data = data\n        self.labels = labels\n        self.transform = transform\n        self.rgb_transform = rgb_transform\n        self.flow_transform = flow_transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # video_path = self.dataframe.iloc[idx]['videos']\n        # label_list = self.dataframe.iloc[idx]['label']\n        video_path = self.data[idx]\n        label = self.labels[idx]\n\n        try:\n            # Extract RGB frames and optical flow\n            parts = video_path.split(\"/\")\n            video_name = parts[-1].split('.')[0]\n            frame_folder = f\"frames/{video_name}\"  # Temporary folder to store frames\n            os.makedirs(frame_folder, exist_ok=True)\n    \n            rgb_frames = extract_frames(video_path, frame_folder)\n            if len(rgb_frames) < 10:\n                # print(f\"Skipping video {video_path} due to insufficient frames.\")\n                return None\n            \n            flow_frames = calculate_optical_flow(rgb_frames)\n            if len(flow_frames) < 1:\n                # print(f\"Skipping video {video_path} due to no optical flow.\")\n                return None\n    \n            # Apply transformations if provided (e.g., resizing, normalization)\n            if self.transform:\n                rgb_frames = [self.rgb_transform(frame) for frame in rgb_frames]  # Use RGB-specific transform\n                flow_frames = [self.flow_transform(flow) for flow in flow_frames]  # Use flow-specific transform\n    \n            # Convert frames to tensors (stacked along the time axis)\n            rgb_tensor = torch.stack([torch.tensor(frame, dtype=torch.float32) for frame in rgb_frames])\n            flow_tensor = torch.stack([torch.tensor(flow, dtype=torch.float32) for flow in flow_frames])\n\n            # label_tensor = torch.tensor(label_list, dtype=torch.float32)  # Convert to tensor\n    \n            return rgb_tensor, flow_tensor, label\n\n        except Exception as e:\n            # print(f\"Error processing video {video_path}: {e}\")\n            return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:37.862729Z","iopub.execute_input":"2024-12-12T10:29:37.863042Z","iopub.status.idle":"2024-12-12T10:29:37.874074Z","shell.execute_reply.started":"2024-12-12T10:29:37.863009Z","shell.execute_reply":"2024-12-12T10:29:37.873144Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Define the Three-Stream Network Architecture\n\n### Spatial and Temporal CNN","metadata":{}},{"cell_type":"code","source":"class CNNStream(nn.Module):\n    def __init__(self, input_channels=3, num_classes=11):\n        super(CNNStream, self).__init__()\n        \n        # Convolutional Block 1\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels=input_channels, out_channels=96, kernel_size=6, stride=2, padding=0),\n            nn.BatchNorm2d(96),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        # Convolutional Block 2\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=2, padding=0),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        \n        # Convolutional Block 3\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n            nn.ReLU()\n        )\n        \n        # Convolutional Block 4\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n            nn.ReLU()\n        )\n        \n        # Convolutional Block 5\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n\n        # Dynamically compute the flattened size\n        self.flatten_size = self._compute_flatten_size(input_channels)\n        \n        # Fully Connected Layers\n        self.fc6 = nn.Sequential(\n            nn.Linear(self.flatten_size, 2048),  # Adjust based on input size\n            nn.Dropout(0.5),\n            nn.ReLU()\n        )\n        self.fc7 = nn.Sequential(\n            nn.Linear(2048, num_classes),\n            nn.Dropout(0.5)\n        )\n\n    def _compute_flatten_size(self, input_channels):\n        \"\"\"\n        Calculate the size of the tensor after convolution and pooling.\n        \"\"\"\n        with torch.no_grad():\n            dummy_input = torch.zeros(1, input_channels, 128, 128)  # Example input size\n            x = self.conv1(dummy_input)\n            x = self.conv2(x)\n            x = self.conv3(x)\n            x = self.conv4(x)\n            x = self.conv5(x)\n            return x.numel()  # Flatten size\n    \n    def forward(self, x):\n        # Reshape to merge batch and sequence dimensions\n        batch_size, seq_len, channels, height, width = x.shape\n        x = x.view(batch_size * seq_len, channels, height, width)  # [batch_size * seq_len, 3, 128, 128]\n        \n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n        \n        # Flatten for fully connected layers\n        x = x.view(x.size(0), -1)\n        x = self.fc6(x)\n        x = self.fc7(x)\n\n        # Reshape back to separate batch and sequence dimensions\n        x = x.view(batch_size, seq_len, -1)  # [batch_size, seq_len, feature_dim]\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:37.875224Z","iopub.execute_input":"2024-12-12T10:29:37.875641Z","iopub.status.idle":"2024-12-12T10:29:37.894996Z","shell.execute_reply.started":"2024-12-12T10:29:37.875595Z","shell.execute_reply":"2024-12-12T10:29:37.894276Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### Sequential LSTM Network","metadata":{}},{"cell_type":"code","source":"class LSTMStream(nn.Module):\n    def __init__(self, input_channels=3, height=128, width=128, hidden_size=256, num_layers=1, num_classes = 11):\n        super(LSTMStream, self).__init__()\n\n        # Calculate the correct input_size by flattening spatial dimensions (channels * height * width)\n        input_size = input_channels * height * width  # This will be the correct input size for LSTM\n        \n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        batch_size, seq_len, channels, height, width = x.shape\n\n        # Reshape the input from [batch_size, seq_len, channels, height, width]\n        x = x.view(batch_size, seq_len, -1)  # Flatten the spatial dimensions (channels * height * width)\n        \n        # Pass through LSTM\n        out, _ = self.lstm(x)\n        out = out[:, -1, :]  # Last time step output\n        out = self.fc(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:37.895889Z","iopub.execute_input":"2024-12-12T10:29:37.896098Z","iopub.status.idle":"2024-12-12T10:29:37.909748Z","shell.execute_reply.started":"2024-12-12T10:29:37.896076Z","shell.execute_reply":"2024-12-12T10:29:37.909031Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Combine the Three Streams","metadata":{}},{"cell_type":"code","source":"class ThreeStreamNetwork(nn.Module):\n    def __init__(self, num_classes = 11):\n        super(ThreeStreamNetwork, self).__init__()\n        self.spatial_stream = CNNStream(input_channels=3, num_classes = 11)\n        self.temporal_stream = CNNStream(input_channels=2, num_classes = 11)  # Optical flow has 2 channels (u, v)\n        self.sequential_stream = LSTMStream(input_channels=3, height=128, width=128, hidden_size=512, num_layers=1, num_classes = 11)\n\n        # self.fc_fusion = nn.Linear(1024 + 1024 + 512, 256)\n        self.fc_mlp = nn.Linear(num_classes, 256)\n        self.fc_output = nn.Linear(256, num_classes)  # 11: For UCF11 dataset\n\n    def forward(self, rgb, flow):\n        spatial_features = self.spatial_stream(rgb)\n        temporal_features = self.temporal_stream(flow)\n\n        sequential_features = self.sequential_stream(rgb)  # For LSTM\n\n        # Option: Average over sequence length (dimension 1)\n        spatial_features_avg = spatial_features.mean(dim=1)  # shape [32, 11]\n        temporal_features_avg = temporal_features.mean(dim=1)  # shape [32, 11]\n        \n        # Sequential features already have shape [32, 11]\n        fused_features = (spatial_features_avg + temporal_features_avg + sequential_features) / 3\n        \n        x = F.relu(self.fc_mlp(fused_features))\n        x = self.fc_output(x)\n        \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:37.910602Z","iopub.execute_input":"2024-12-12T10:29:37.910832Z","iopub.status.idle":"2024-12-12T10:29:37.922390Z","shell.execute_reply.started":"2024-12-12T10:29:37.910807Z","shell.execute_reply":"2024-12-12T10:29:37.921540Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def get_kfold_data(dataset, labels, n_splits=10, shuffle=True):\n    \"\"\"\n    Split dataset into k folds.\n    Args:\n        dataset: List or array of data.\n        labels: Corresponding labels.\n        n_splits: Number of folds for cross-validation.\n        shuffle: Whether to shuffle the data before splitting.\n    Returns:\n        List of train-test splits.\n    \"\"\"\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=42)\n    folds = []\n    \n    for train_idx, test_idx in skf.split(dataset, labels):\n        train_data = [dataset[i] for i in train_idx]\n        test_data = [dataset[i] for i in test_idx]\n        train_labels = [labels[i] for i in train_idx]\n        test_labels = [labels[i] for i in test_idx]\n        folds.append((train_data, train_labels, test_data, test_labels))\n    \n    return folds\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:37.923389Z","iopub.execute_input":"2024-12-12T10:29:37.923659Z","iopub.status.idle":"2024-12-12T10:29:37.937510Z","shell.execute_reply.started":"2024-12-12T10:29:37.923635Z","shell.execute_reply":"2024-12-12T10:29:37.936750Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_loader, criterion, optimizer, device, num_epochs=10):\n    model.train()\n    \n    for epoch in range(num_epochs):\n        \n        start_event = torch.cuda.Event(enable_timing=True)\n        end_event = torch.cuda.Event(enable_timing=True)\n        start_event.record()\n        \n        running_loss = 0.0 \n        train_accuracy = 0.0\n        correct = 0\n        total = 0\n\n        for batch in train_loader:\n            if batch is None:  # Skip empty batches\n                continue\n            rgb, flow, labels = batch\n            rgb, flow, labels = rgb.to(device), flow.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(rgb, flow)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n        train_accuracy = correct / total\n\n        end_event.record()\n        torch.cuda.synchronize()  # Wait for all GPU operations to finish\n        training_time = start_event.elapsed_time(end_event) / 1000  # Convert to seconds\n        print(\"Epoch time: {:.2f} seconds\".format(training_time))\n\n        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}, Train Accuracy: {train_accuracy:.4f} \")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:37.938416Z","iopub.execute_input":"2024-12-12T10:29:37.938677Z","iopub.status.idle":"2024-12-12T10:29:37.953721Z","shell.execute_reply.started":"2024-12-12T10:29:37.938649Z","shell.execute_reply":"2024-12-12T10:29:37.953002Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, test_loader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for batch in test_loader:\n            if batch is None:  # Skip empty batches\n                continue\n            rgb, flow, labels = batch\n            rgb, flow, labels = rgb.to(device), flow.to(device), labels.to(device)\n            \n            outputs = model(rgb, flow)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n            \n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n    \n    accuracy = correct / total\n    avg_loss = total_loss / len(test_loader)\n    \n    return accuracy, avg_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:37.954800Z","iopub.execute_input":"2024-12-12T10:29:37.955038Z","iopub.status.idle":"2024-12-12T10:29:37.967866Z","shell.execute_reply.started":"2024-12-12T10:29:37.955015Z","shell.execute_reply":"2024-12-12T10:29:37.967086Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def safe_collate_fn(batch):\n    \"\"\"Remove None samples from the batch.\"\"\"\n    batch = [b for b in batch if b is not None]\n    if not batch:\n        return None  # Return None if the batch is empty\n    return default_collate(batch)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:37.968882Z","iopub.execute_input":"2024-12-12T10:29:37.969212Z","iopub.status.idle":"2024-12-12T10:29:37.982117Z","shell.execute_reply.started":"2024-12-12T10:29:37.969184Z","shell.execute_reply":"2024-12-12T10:29:37.981390Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# Cross Validation","metadata":{}},{"cell_type":"code","source":"def cross_validate(model_class, num_classes, dataset, labels, n_splits=10, device=\"cuda\"):\n    \"\"\"\n    Perform k-fold cross-validation.\n    Args:\n        model_class: The neural network class to be instantiated.\n        dataset: Full dataset.\n        labels: Corresponding labels.\n        n_splits: Number of folds for cross-validation.\n        device: Device to run the training (e.g., \"cuda\" or \"cpu\").\n    Returns:\n        Average accuracy and loss across folds.\n    \"\"\"\n    folds = get_kfold_data(dataset, labels, n_splits=n_splits)\n    fold_accuracies = []\n    fold_losses = []\n\n\n    # Define transforms\n    rgb_transform = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.Resize((128, 128)),  # Resize frames to 128x128\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # For RGB normalization\n    ])\n    \n    # For optical flow (u, v channels)\n    flow_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Resize((128, 128)), # Resize frames to 128x128\n        transforms.Normalize(mean=[0.5, 0.5], std=[0.5, 0.5])  # 2 channels\n    ])\n\n\n    for fold_idx, (train_data, train_labels, test_data, test_labels) in enumerate(folds):\n        print(f\"Starting fold {fold_idx + 1}/{n_splits}...\")\n        \n        # Prepare data loaders\n        train_dataset = VideoDataset(data=train_data, \n                                     labels=train_labels, \n                                     transform=True,\n                                     rgb_transform=rgb_transform, \n                                     flow_transform=flow_transform)\n        test_dataset = VideoDataset(data=test_data,\n                                    labels=test_labels,\n                                    transform=True,\n                                    rgb_transform=rgb_transform, \n                                    flow_transform=flow_transform)\n        \n        train_loader = DataLoader(train_dataset, batch_size=64, collate_fn=safe_collate_fn, shuffle=True)\n        test_loader = DataLoader(test_dataset, batch_size=64, collate_fn=safe_collate_fn, shuffle=False)\n\n\n        # Initialize model, loss function, and optimizer\n        model = model_class(num_classes=num_classes).to(device)\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.000125)\n\n\n        # Train the model\n        train_model(model, train_loader, criterion, optimizer, device)\n\n        # Evaluate the model\n        accuracy, loss = evaluate_model(model, test_loader, criterion, device)\n        fold_accuracies.append(accuracy)\n        fold_losses.append(loss)\n\n        print(f\"Fold {fold_idx + 1} - Accuracy: {accuracy:.4f}, Loss: {loss:.4f}\")\n\n    # Compute average performance\n    avg_accuracy = np.mean(fold_accuracies)\n    avg_loss = np.mean(fold_losses)\n    print(f\"10-Fold CV - Average Accuracy: {avg_accuracy:.4f}, Average Loss: {avg_loss:.4f}\")\n    \n    return avg_accuracy, avg_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:37.983267Z","iopub.execute_input":"2024-12-12T10:29:37.983506Z","iopub.status.idle":"2024-12-12T10:29:37.995439Z","shell.execute_reply.started":"2024-12-12T10:29:37.983482Z","shell.execute_reply":"2024-12-12T10:29:37.994748Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Hyperparameters and setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n#Additional Info when using cuda\nif device.type == 'cuda':\n    print(torch.cuda.get_device_name(0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:37.996434Z","iopub.execute_input":"2024-12-12T10:29:37.996707Z","iopub.status.idle":"2024-12-12T10:29:38.066468Z","shell.execute_reply.started":"2024-12-12T10:29:37.996682Z","shell.execute_reply":"2024-12-12T10:29:38.065568Z"}},"outputs":[{"name":"stdout","text":"cuda\nTesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"data = df['videos']\nlabels = df['label']\n\n# Assuming dataset and labels are preprocessed lists\navg_accuracy, avg_loss = cross_validate(ThreeStreamNetwork, num_classes, data, labels, n_splits=10, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T10:29:38.067538Z","iopub.execute_input":"2024-12-12T10:29:38.067913Z","iopub.status.idle":"2024-12-12T20:06:40.286918Z","shell.execute_reply.started":"2024-12-12T10:29:38.067874Z","shell.execute_reply":"2024-12-12T20:06:40.285943Z"}},"outputs":[{"name":"stdout","text":"Starting fold 1/10...\nEpoch time: 365.11 seconds\nEpoch [1/10], Loss: 2.3205, Train Accuracy: 0.1573 \nEpoch time: 346.04 seconds\nEpoch [2/10], Loss: 1.9084, Train Accuracy: 0.3013 \nEpoch time: 344.75 seconds\nEpoch [3/10], Loss: 1.6725, Train Accuracy: 0.3890 \nEpoch time: 350.02 seconds\nEpoch [4/10], Loss: 1.5077, Train Accuracy: 0.4600 \nEpoch time: 349.07 seconds\nEpoch [5/10], Loss: 1.3314, Train Accuracy: 0.5198 \nEpoch time: 349.17 seconds\nEpoch [6/10], Loss: 1.1507, Train Accuracy: 0.5894 \nEpoch time: 344.84 seconds\nEpoch [7/10], Loss: 1.0334, Train Accuracy: 0.6360 \nEpoch time: 345.41 seconds\nEpoch [8/10], Loss: 0.9238, Train Accuracy: 0.6771 \nEpoch time: 344.32 seconds\nEpoch [9/10], Loss: 0.8211, Train Accuracy: 0.7244 \nEpoch time: 344.22 seconds\nEpoch [10/10], Loss: 0.7824, Train Accuracy: 0.7370 \nFold 1 - Accuracy: 0.6937, Loss: 1.0712\nStarting fold 2/10...\nEpoch time: 345.38 seconds\nEpoch [1/10], Loss: 2.2802, Train Accuracy: 0.1907 \nEpoch time: 339.97 seconds\nEpoch [2/10], Loss: 1.9899, Train Accuracy: 0.2756 \nEpoch time: 337.45 seconds\nEpoch [3/10], Loss: 1.7271, Train Accuracy: 0.3667 \nEpoch time: 342.15 seconds\nEpoch [4/10], Loss: 1.5350, Train Accuracy: 0.4391 \nEpoch time: 343.43 seconds\nEpoch [5/10], Loss: 1.2960, Train Accuracy: 0.5518 \nEpoch time: 344.32 seconds\nEpoch [6/10], Loss: 1.2064, Train Accuracy: 0.5658 \nEpoch time: 343.32 seconds\nEpoch [7/10], Loss: 1.0751, Train Accuracy: 0.6152 \nEpoch time: 342.30 seconds\nEpoch [8/10], Loss: 0.9658, Train Accuracy: 0.6708 \nEpoch time: 342.94 seconds\nEpoch [9/10], Loss: 0.8622, Train Accuracy: 0.7133 \nEpoch time: 343.02 seconds\nEpoch [10/10], Loss: 0.8208, Train Accuracy: 0.7084 \nFold 2 - Accuracy: 0.6750, Loss: 0.9192\nStarting fold 3/10...\nEpoch time: 345.97 seconds\nEpoch [1/10], Loss: 2.2346, Train Accuracy: 0.1809 \nEpoch time: 342.67 seconds\nEpoch [2/10], Loss: 1.8534, Train Accuracy: 0.3132 \nEpoch time: 345.09 seconds\nEpoch [3/10], Loss: 1.6921, Train Accuracy: 0.3772 \nEpoch time: 341.35 seconds\nEpoch [4/10], Loss: 1.5790, Train Accuracy: 0.4182 \nEpoch time: 341.36 seconds\nEpoch [5/10], Loss: 1.4426, Train Accuracy: 0.4871 \nEpoch time: 340.78 seconds\nEpoch [6/10], Loss: 1.3526, Train Accuracy: 0.5414 \nEpoch time: 340.65 seconds\nEpoch [7/10], Loss: 1.2024, Train Accuracy: 0.5818 \nEpoch time: 338.79 seconds\nEpoch [8/10], Loss: 1.0843, Train Accuracy: 0.6291 \nEpoch time: 340.13 seconds\nEpoch [9/10], Loss: 1.0256, Train Accuracy: 0.6701 \nEpoch time: 342.36 seconds\nEpoch [10/10], Loss: 0.8234, Train Accuracy: 0.7244 \nFold 3 - Accuracy: 0.6687, Loss: 0.9450\nStarting fold 4/10...\nEpoch time: 339.37 seconds\nEpoch [1/10], Loss: 2.2574, Train Accuracy: 0.1949 \nEpoch time: 338.53 seconds\nEpoch [2/10], Loss: 1.9406, Train Accuracy: 0.3111 \nEpoch time: 338.01 seconds\nEpoch [3/10], Loss: 1.6971, Train Accuracy: 0.3967 \nEpoch time: 339.08 seconds\nEpoch [4/10], Loss: 1.4755, Train Accuracy: 0.4809 \nEpoch time: 339.46 seconds\nEpoch [5/10], Loss: 1.3017, Train Accuracy: 0.5351 \nEpoch time: 341.78 seconds\nEpoch [6/10], Loss: 1.1689, Train Accuracy: 0.5971 \nEpoch time: 344.08 seconds\nEpoch [7/10], Loss: 1.0564, Train Accuracy: 0.6416 \nEpoch time: 341.64 seconds\nEpoch [8/10], Loss: 0.9262, Train Accuracy: 0.6757 \nEpoch time: 343.59 seconds\nEpoch [9/10], Loss: 0.8655, Train Accuracy: 0.7070 \nEpoch time: 343.63 seconds\nEpoch [10/10], Loss: 0.7636, Train Accuracy: 0.7488 \nFold 4 - Accuracy: 0.7438, Loss: 0.7316\nStarting fold 5/10...\nEpoch time: 346.02 seconds\nEpoch [1/10], Loss: 2.3113, Train Accuracy: 0.1997 \nEpoch time: 342.67 seconds\nEpoch [2/10], Loss: 1.8917, Train Accuracy: 0.3479 \nEpoch time: 343.52 seconds\nEpoch [3/10], Loss: 1.5390, Train Accuracy: 0.4308 \nEpoch time: 343.78 seconds\nEpoch [4/10], Loss: 1.4462, Train Accuracy: 0.4662 \nEpoch time: 345.61 seconds\nEpoch [5/10], Loss: 1.3648, Train Accuracy: 0.5254 \nEpoch time: 342.45 seconds\nEpoch [6/10], Loss: 1.1924, Train Accuracy: 0.5769 \nEpoch time: 342.22 seconds\nEpoch [7/10], Loss: 1.1001, Train Accuracy: 0.6061 \nEpoch time: 342.06 seconds\nEpoch [8/10], Loss: 0.9945, Train Accuracy: 0.6458 \nEpoch time: 342.47 seconds\nEpoch [9/10], Loss: 0.8984, Train Accuracy: 0.6910 \nEpoch time: 342.03 seconds\nEpoch [10/10], Loss: 0.7937, Train Accuracy: 0.7307 \nFold 5 - Accuracy: 0.7312, Loss: 0.9302\nStarting fold 6/10...\nEpoch time: 339.29 seconds\nEpoch [1/10], Loss: 2.2923, Train Accuracy: 0.1822 \nEpoch time: 343.24 seconds\nEpoch [2/10], Loss: 1.9938, Train Accuracy: 0.2761 \nEpoch time: 343.27 seconds\nEpoch [3/10], Loss: 1.6650, Train Accuracy: 0.4040 \nEpoch time: 346.84 seconds\nEpoch [4/10], Loss: 1.5401, Train Accuracy: 0.4471 \nEpoch time: 341.32 seconds\nEpoch [5/10], Loss: 1.4108, Train Accuracy: 0.5056 \nEpoch time: 340.98 seconds\nEpoch [6/10], Loss: 1.2436, Train Accuracy: 0.5654 \nEpoch time: 339.33 seconds\nEpoch [7/10], Loss: 1.1142, Train Accuracy: 0.6120 \nEpoch time: 339.49 seconds\nEpoch [8/10], Loss: 0.9549, Train Accuracy: 0.6620 \nEpoch time: 342.87 seconds\nEpoch [9/10], Loss: 0.8908, Train Accuracy: 0.6850 \nEpoch time: 341.94 seconds\nEpoch [10/10], Loss: 0.8317, Train Accuracy: 0.7135 \nFold 6 - Accuracy: 0.7484, Loss: 0.7671\nStarting fold 7/10...\nEpoch time: 342.29 seconds\nEpoch [1/10], Loss: 2.2868, Train Accuracy: 0.1592 \nEpoch time: 341.69 seconds\nEpoch [2/10], Loss: 1.9670, Train Accuracy: 0.2823 \nEpoch time: 340.50 seconds\nEpoch [3/10], Loss: 1.7229, Train Accuracy: 0.3783 \nEpoch time: 339.80 seconds\nEpoch [4/10], Loss: 1.5423, Train Accuracy: 0.4395 \nEpoch time: 338.65 seconds\nEpoch [5/10], Loss: 1.4047, Train Accuracy: 0.4819 \nEpoch time: 340.34 seconds\nEpoch [6/10], Loss: 1.3025, Train Accuracy: 0.5334 \nEpoch time: 339.12 seconds\nEpoch [7/10], Loss: 1.2089, Train Accuracy: 0.5473 \nEpoch time: 339.87 seconds\nEpoch [8/10], Loss: 1.0781, Train Accuracy: 0.6273 \nEpoch time: 340.94 seconds\nEpoch [9/10], Loss: 0.9726, Train Accuracy: 0.6523 \nEpoch time: 339.76 seconds\nEpoch [10/10], Loss: 0.9141, Train Accuracy: 0.6905 \nFold 7 - Accuracy: 0.6604, Loss: 1.0091\nStarting fold 8/10...\nEpoch time: 340.58 seconds\nEpoch [1/10], Loss: 2.2847, Train Accuracy: 0.1780 \nEpoch time: 337.27 seconds\nEpoch [2/10], Loss: 1.9206, Train Accuracy: 0.3268 \nEpoch time: 339.95 seconds\nEpoch [3/10], Loss: 1.5850, Train Accuracy: 0.4228 \nEpoch time: 341.29 seconds\nEpoch [4/10], Loss: 1.4451, Train Accuracy: 0.4910 \nEpoch time: 340.78 seconds\nEpoch [5/10], Loss: 1.2873, Train Accuracy: 0.5445 \nEpoch time: 340.47 seconds\nEpoch [6/10], Loss: 1.1435, Train Accuracy: 0.6015 \nEpoch time: 340.52 seconds\nEpoch [7/10], Loss: 1.0297, Train Accuracy: 0.6419 \nEpoch time: 343.14 seconds\nEpoch [8/10], Loss: 0.8940, Train Accuracy: 0.6878 \nEpoch time: 342.78 seconds\nEpoch [9/10], Loss: 0.8458, Train Accuracy: 0.7065 \nEpoch time: 344.51 seconds\nEpoch [10/10], Loss: 0.7715, Train Accuracy: 0.7371 \nFold 8 - Accuracy: 0.7925, Loss: 0.7408\nStarting fold 9/10...\nEpoch time: 345.35 seconds\nEpoch [1/10], Loss: 2.3035, Train Accuracy: 0.1823 \nEpoch time: 342.45 seconds\nEpoch [2/10], Loss: 2.0445, Train Accuracy: 0.3027 \nEpoch time: 341.80 seconds\nEpoch [3/10], Loss: 1.7320, Train Accuracy: 0.3897 \nEpoch time: 341.86 seconds\nEpoch [4/10], Loss: 1.5622, Train Accuracy: 0.4489 \nEpoch time: 343.24 seconds\nEpoch [5/10], Loss: 1.3873, Train Accuracy: 0.4955 \nEpoch time: 341.87 seconds\nEpoch [6/10], Loss: 1.1977, Train Accuracy: 0.5901 \nEpoch time: 344.87 seconds\nEpoch [7/10], Loss: 1.0693, Train Accuracy: 0.6291 \nEpoch time: 340.73 seconds\nEpoch [8/10], Loss: 0.9163, Train Accuracy: 0.6896 \nEpoch time: 343.86 seconds\nEpoch [9/10], Loss: 0.8382, Train Accuracy: 0.7279 \nEpoch time: 342.87 seconds\nEpoch [10/10], Loss: 0.7340, Train Accuracy: 0.7509 \nFold 9 - Accuracy: 0.5938, Loss: 1.3585\nStarting fold 10/10...\nEpoch time: 341.86 seconds\nEpoch [1/10], Loss: 2.2846, Train Accuracy: 0.1942 \nEpoch time: 341.15 seconds\nEpoch [2/10], Loss: 1.8861, Train Accuracy: 0.3319 \nEpoch time: 344.63 seconds\nEpoch [3/10], Loss: 1.6227, Train Accuracy: 0.4175 \nEpoch time: 341.60 seconds\nEpoch [4/10], Loss: 1.5068, Train Accuracy: 0.4447 \nEpoch time: 339.49 seconds\nEpoch [5/10], Loss: 1.3428, Train Accuracy: 0.5177 \nEpoch time: 338.09 seconds\nEpoch [6/10], Loss: 1.1878, Train Accuracy: 0.5692 \nEpoch time: 341.05 seconds\nEpoch [7/10], Loss: 1.0640, Train Accuracy: 0.6291 \nEpoch time: 339.10 seconds\nEpoch [8/10], Loss: 0.9204, Train Accuracy: 0.6792 \nEpoch time: 338.33 seconds\nEpoch [9/10], Loss: 0.8086, Train Accuracy: 0.7293 \nEpoch time: 337.96 seconds\nEpoch [10/10], Loss: 0.7512, Train Accuracy: 0.7307 \nFold 10 - Accuracy: 0.7688, Loss: 0.7892\n10-Fold CV - Average Accuracy: 0.7076, Average Loss: 0.9262\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}